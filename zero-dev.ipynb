{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f63a0df57d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConnectFour:\n",
    "    def __init__(self):\n",
    "        self.row_count = 6\n",
    "        self.column_count = 7\n",
    "        self.action_size = self.column_count\n",
    "        self.in_a_row = 4\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"ConnectFour\"\n",
    "    \n",
    "    def get_initial_state(self):\n",
    "        return np.zeros((self.row_count, self.column_count))\n",
    "    \n",
    "    def get_next_state(self, state, action, player):\n",
    "        row = np.max((np.where(state[:, action] == 0)))\n",
    "        state[row, action] = player\n",
    "        return state\n",
    "    \n",
    "    def get_valid_moves(self, state):\n",
    "        return (state[0] == 0).astype(np.uint8)\n",
    "    \n",
    "    def check_win(self, state, action):\n",
    "        # At root node\n",
    "        if action == None:\n",
    "            return False\n",
    "        \n",
    "        row = np.min(np.where(state[:, action] != 0))\n",
    "        column = action\n",
    "        player = state[row][column]\n",
    "\n",
    "        def count(offset_row, offset_column):\n",
    "            for i in range(1, self.in_a_row):\n",
    "                r = row + offset_row * i\n",
    "                c = action + offset_column * i\n",
    "                if (\n",
    "                    r < 0\n",
    "                    or r >= self.row_count\n",
    "                    or c < 0\n",
    "                    or c >= self.column_count\n",
    "                    or state[r][c] != player\n",
    "                ):\n",
    "                    return i - 1\n",
    "            return self.in_a_row - 1\n",
    "        \n",
    "        win = (\n",
    "            count(1, 0) >= self.in_a_row - 1 # vertical\n",
    "            or (count(0, 1) + count(0, -1)) >= self.in_a_row - 1 # horizontal\n",
    "            or (count(1, 1) + count(-1, -1)) >= self.in_a_row - 1 # top left diagonal\n",
    "            or (count(-1, -1) + count(1, 1) >= self.in_a_row - 1) # top right diagonal\n",
    "        )\n",
    "        return win\n",
    "\n",
    "\n",
    "    def get_value_and_terminated(self, state, action) -> tuple:\n",
    "        if self.check_win(state, action):\n",
    "            return 1, True\n",
    "        if np.sum(self.get_valid_moves(state)) == 0:\n",
    "            return 0, True\n",
    "        return 0, False\n",
    "        \n",
    "\n",
    "    def get_opponent(self, player):\n",
    "        return -player\n",
    "    \n",
    "    def get_opponent_value(self, value):\n",
    "        return -value\n",
    "    \n",
    "    def change_perspective(self, state, player):\n",
    "        return state * player\n",
    "    \n",
    "    def get_encoded_state(self, state):\n",
    "        encoded_state = np.stack(\n",
    "            (state == -1, state == 0, state == 1)\n",
    "        ).astype(np.float32)\n",
    "\n",
    "        if len(state.shape) == 3:\n",
    "            encoded_state = np.swapaxes(encoded_state, 0, 1)\n",
    "\n",
    "\n",
    "        return encoded_state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToe:\n",
    "    def __init__(self):\n",
    "        self.row_count = 3\n",
    "        self.column_count = 3\n",
    "        self.action_size = self.row_count * self.column_count\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"TicTacToe\"\n",
    "    \n",
    "    \n",
    "    def get_initial_state(self):\n",
    "        return np.zeros((self.row_count, self.column_count))\n",
    "    \n",
    "    def get_next_state(self, state, action, player):\n",
    "        row = action // self.column_count\n",
    "        column = action % self.row_count\n",
    "        state[row, column] = player\n",
    "        return state\n",
    "    \n",
    "    def get_valid_moves(self, state):\n",
    "        return (state.reshape(-1) == 0).astype(np.uint8)\n",
    "    \n",
    "    def check_win(self, state, action):\n",
    "        # At root node\n",
    "        if action == None:\n",
    "            return False\n",
    "        row = action // self.column_count\n",
    "        column = action % self.column_count\n",
    "        player = state[row, column]\n",
    "        win = (\n",
    "            np.sum(state[row, :]) == player * self.column_count\n",
    "            or np.sum(state[:, column]) == player * self.row_count\n",
    "            or np.sum(np.diag(state)) == player * self.row_count\n",
    "            or np.sum(np.diag(np.flip(state, axis=1))) == player * self.row_count\n",
    "        )\n",
    "        return win\n",
    "\n",
    "\n",
    "    def get_value_and_terminated(self, state, action) -> tuple:\n",
    "        if self.check_win(state, action):\n",
    "            return 1, True\n",
    "        \n",
    "        if np.sum(self.get_valid_moves(state)) == 0:\n",
    "            return 0, True\n",
    "        \n",
    "        return 0, False\n",
    "        \n",
    "\n",
    "    def get_opponent(self, player):\n",
    "        return -player\n",
    "    \n",
    "    def get_opponent_value(self, value):\n",
    "        return -value\n",
    "    \n",
    "    def change_perspective(self, state, player):\n",
    "        return state * player\n",
    "    \n",
    "    def get_encoded_state(self, state):\n",
    "        encoded_state = np.stack(\n",
    "            (state == -1, state == 0, state == 1)\n",
    "        ).astype(np.float32)\n",
    "\n",
    "        if len(state.shape) == 3:\n",
    "            # encoded_state is (3, batch, height, width); swap to (batch, 3, height, width)\n",
    "            encoded_state = np.transpose(encoded_state, (1, 0, 2, 3))\n",
    "\n",
    "        return encoded_state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, game, num_resBlocks, num_hidden, device):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.device = device\n",
    "        self.start_block = nn.Sequential(\n",
    "              nn.Conv2d(3, num_hidden, kernel_size=3, padding=1),\n",
    "              nn.BatchNorm2d(num_hidden),\n",
    "              nn.ReLU()\n",
    "         )\n",
    "        self.backBone = nn.ModuleList(\n",
    "            [ResBlock(num_hidden) for _ in range(num_resBlocks)]\n",
    "        )\n",
    "        self.policyHead = nn.Sequential(\n",
    "            nn.Conv2d(num_hidden, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32 * game.row_count * game.column_count, game.action_size)\n",
    "        )\n",
    "        self.valueHead = nn.Sequential(\n",
    "            nn.Conv2d(num_hidden, 3, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(3),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3 * game.row_count * game.column_count, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.start_block(x)\n",
    "        for ResBlock in self.backBone:\n",
    "            x = ResBlock(x)\n",
    "        policy = self.policyHead(x)\n",
    "        value = self.valueHead(x)\n",
    "        return policy, value\n",
    "        \n",
    "\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, num_hidden):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(num_hidden)\n",
    "        self.conv2 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(num_hidden)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        x = F.relu(x + residual)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, game, args, state, parent=None, action_taken=None, prior=0, visit_count=0):\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.state = state\n",
    "        self.parent = parent\n",
    "        self.action_taken = action_taken\n",
    "        self.prior = prior\n",
    "\n",
    "        self.children = []\n",
    "        \n",
    "        self.visit_count = visit_count\n",
    "        self.value_sum = 0\n",
    "\n",
    "    def is_fully_expanded(self):\n",
    "        return len(self.children) > 0\n",
    "\n",
    "    def select(self):\n",
    "        best_child = None\n",
    "        best_ucb = -np.inf\n",
    "\n",
    "        for child in self.children:\n",
    "            ucb = self.get_ucb(child)\n",
    "\n",
    "            if ucb > best_ucb:\n",
    "                best_child = child\n",
    "                best_ucb = ucb\n",
    "\n",
    "        return best_child\n",
    "    \n",
    "    def get_ucb(self, child):\n",
    "        if child.visit_count == 0:\n",
    "            q_value = 0\n",
    "        else:\n",
    "            q_value = 1 - ((child.value_sum / child.visit_count) + 1) / 2\n",
    "        return q_value + self.args['C'] * math.sqrt(\n",
    "            self.visit_count / (child.visit_count + 1)) * child.prior\n",
    "    \n",
    "\n",
    "    def expand(self, policy):\n",
    "        for action, prob in enumerate(policy):\n",
    "            if prob > 0:\n",
    "                child_state = self.state.copy()\n",
    "                player = 1\n",
    "                child_state = self.game.get_next_state(child_state, action, player=player)\n",
    "                child_state = self.game.change_perspective(child_state, player=-player)\n",
    "                child = Node(self.game, self.args, child_state, self, action, prob)\n",
    "                self.children.append(child)\n",
    "\n",
    "        return child\n",
    "\n",
    "    def backpropagate(self, value):\n",
    "        self.value_sum += value\n",
    "        self.visit_count += 1\n",
    "\n",
    "        value = self.game.get_opponent_value(value)\n",
    "        if self.parent is not None:\n",
    "           self.parent.backpropagate(value)\n",
    "\n",
    "\n",
    "class MCTS:\n",
    "    def __init__(self, game, args, model):\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.model = model\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def search(self, state):\n",
    "        root = Node(self.game, self.args, state, visit_count=1)\n",
    "\n",
    "        policy, _ = self.model(\n",
    "            torch.tensor(self.game.get_encoded_state(state), device=self.model.device).unsqueeze(0)\n",
    "        )\n",
    "        policy = torch.softmax(policy, dim=1).squeeze(0).cpu().numpy()\n",
    "        policy = (1 - self.args['dirichlet_epsilon']) * policy + self.args['dirichlet_epsilon'] \\\n",
    "            * np.random.dirichlet([self.args['dirichlet_alpha']] * self.game.action_size)\n",
    "        \n",
    "        valid_moves = self.game.get_valid_moves(state)\n",
    "        policy *= valid_moves\n",
    "        policy /= np.sum(policy)\n",
    "\n",
    "        root.expand(policy)\n",
    "        \n",
    "        for search in range(self.args['num_searches']):\n",
    "            # selection\n",
    "            node = root\n",
    "\n",
    "            # expansion\n",
    "            while node.is_fully_expanded():\n",
    "                node = node.select()\n",
    "\n",
    "            value, is_terminal = self.game.get_value_and_terminated(\n",
    "                node.state, node.action_taken)\n",
    "            value = self.game.get_opponent_value(value)\n",
    "\n",
    "            if not is_terminal:\n",
    "                policy, value = self.model(\n",
    "                    torch.tensor(self.game.get_encoded_state(node.state), device=self.model.device).unsqueeze(0)\n",
    "                )\n",
    "                policy = torch.softmax(policy, dim=1).squeeze(0).cpu().numpy()\n",
    "                valid_moves = self.game.get_valid_moves(node.state)\n",
    "                policy *= valid_moves\n",
    "                policy /= np.sum(policy)\n",
    "\n",
    "                value = value.item()\n",
    "\n",
    "                node.expand(policy)\n",
    "                \n",
    "            # backprop\n",
    "            node.backpropagate(value)\n",
    "\n",
    "        # return visit_counts\n",
    "        action_probs = np.zeros(self.game.action_size)\n",
    "        for child in root.children:\n",
    "            action_probs[child.action_taken] = child.visit_count\n",
    "        action_probs /= np.sum(action_probs)\n",
    "        return action_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaZero:\n",
    "    def __init__(self, model, optimizer, game, args):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.mcts = MCTS(game, args, model)\n",
    "        \n",
    "\n",
    "    def self_play(self):\n",
    "        memory = []\n",
    "        player = 1\n",
    "        state = self.game.get_initial_state()\n",
    "\n",
    "        while True:\n",
    "            neutral_state = self.game.change_perspective(state, player)\n",
    "            action_probs = self.mcts.search(neutral_state)\n",
    "\n",
    "            memory.append((neutral_state, action_probs, player))\n",
    "\n",
    "            temperature_action_probs = action_probs ** (1 / self.args['temperature'])\n",
    "            temperature_action_probs /= np.sum(temperature_action_probs)\n",
    "\n",
    "            # changed to temp_probs, but video has action_probs\n",
    "            action = np.random.choice(self.game.action_size, p=temperature_action_probs)\n",
    "            state = self.game.get_next_state(state, action, player)\n",
    "\n",
    "            value, is_terminal = self.game.get_value_and_terminated(state, action)\n",
    "\n",
    "            if is_terminal:\n",
    "                return_memory = []\n",
    "                for hist_neutral_state, hist_action_probs, hist_player in memory:\n",
    "                    hist_outcome = (value if hist_player == player else self.game.get_opponent_value(value))\n",
    "                    return_memory.append((\n",
    "                        self.game.get_encoded_state(hist_neutral_state),\n",
    "                        hist_action_probs,\n",
    "                        hist_outcome\n",
    "                    ))\n",
    "                return return_memory\n",
    "            \n",
    "            player = self.game.get_opponent(player)\n",
    "\n",
    "    def train(self, memory):\n",
    "        random.shuffle(memory)\n",
    "        for batch_idx in range(0, len(memory), self.args['batch_size']):\n",
    "            sample = memory[batch_idx:min(len(memory) - 1, batch_idx + self.args['batch_size'])]\n",
    "            state, policy_targets, value_targets = zip(*sample)\n",
    "            state, policy_targets, value_targets = np.array(state), np.array(policy_targets), np.array(value_targets).reshape(-1, 1)\n",
    "            \n",
    "            state = torch.tensor(state, dtype=torch.float32, device=self.model.device)\n",
    "            policy_targets = torch.tensor(policy_targets, dtype=torch.float32, device=self.model.device)\n",
    "            value_targets = torch.tensor(value_targets, dtype=torch.float32, device=self.model.device)\n",
    "\n",
    "            out_policy, out_value = self.model(state)\n",
    "            \n",
    "            policy_loss = F.cross_entropy(out_policy, policy_targets)\n",
    "            value_loss = F.mse_loss(out_value, value_targets)\n",
    "            loss = policy_loss + value_loss\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "\n",
    "    def learn(self):\n",
    "        for i in range(self.args['num_iterations']):\n",
    "            memory = []\n",
    "            self.model.eval()\n",
    "            for self_play_iteration in tqdm(range(self.args['num_self_play_iterations'])):\n",
    "                memory += self.self_play()\n",
    "\n",
    "            self.model.train()\n",
    "            for eopch in tqdm(range(self.args['num_epochs'])):\n",
    "                self.train(memory)\n",
    "\n",
    "            torch.save(self.model.state_dict(), f\"weights/model_{i}_{self.game}.pt\")\n",
    "            torch.save(self.optimizer.state_dict(), f\"weights/optimizer_{i}_{self.game}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTSParallel:\n",
    "    def __init__(self, game, args, model):\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.model = model\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def search(self, states, self_play_games):\n",
    "        policy, _ = self.model(\n",
    "            torch.tensor(self.game.get_encoded_state(states), device=self.model.device)\n",
    "        )\n",
    "        policy = torch.softmax(policy, dim=1).cpu().numpy()\n",
    "        policy = (1 - self.args['dirichlet_epsilon']) * policy + self.args['dirichlet_epsilon'] \\\n",
    "            * np.random.dirichlet([self.args['dirichlet_alpha']] * self.game.action_size, size=policy.shape[0])\n",
    "        \n",
    "\n",
    "        for i, spg in enumerate(self_play_games):\n",
    "            spg_policy = policy[i]    \n",
    "            valid_moves = self.game.get_valid_moves(states[i])\n",
    "            spg_policy *= valid_moves\n",
    "            # spg_policy /= np.sum(spg_policy )\n",
    "            sum_spg = np.sum(spg_policy)\n",
    "            if sum_spg == 0:\n",
    "                spg_policy = valid_moves.astype(np.float32)\n",
    "            else:\n",
    "                spg_policy /= sum_spg\n",
    "            \n",
    "\n",
    "            spg.root = Node(self.game, self.args, states[i], visit_count=1)\n",
    "            spg.root.expand(spg_policy)\n",
    "         \n",
    "        for search in range(self.args['num_searches']):\n",
    "            for spg in self_play_games:\n",
    "                spg.node = None\n",
    "                # selection\n",
    "                node = spg.root\n",
    "\n",
    "            # expansion\n",
    "            while node.is_fully_expanded():\n",
    "                node = node.select()\n",
    "\n",
    "                value, is_terminal = self.game.get_value_and_terminated(\n",
    "                    node.state, node.action_taken)\n",
    "                value = self.game.get_opponent_value(value)\n",
    "\n",
    "                if is_terminal:\n",
    "                    # backprop\n",
    "                    node.backpropagate(value)\n",
    "                else:\n",
    "                    spg.node = node\n",
    "\n",
    "            expandable_self_play_games = [mapping_idx for mapping_idx in range(len(self_play_games)) if self_play_games[mapping_idx].node is not None]\n",
    "\n",
    "            # truthy not empty list\n",
    "            # if buggy, try len(e_s_p_g) > 0:\n",
    "            if expandable_self_play_games:\n",
    "                states = np.stack([self_play_games[mapping_idx].node.state for mapping_idx in expandable_self_play_games])\n",
    "                policy, value = self.model(\n",
    "                    torch.tensor(self.game.get_encoded_state(states), device=self.model.device)\n",
    "                )\n",
    "                policy = torch.softmax(policy, dim=1).cpu().numpy()\n",
    "                value = value.cpu().numpy()\n",
    "\n",
    "            for i, mapping_idx in enumerate(expandable_self_play_games):\n",
    "                node = self_play_games[mapping_idx].node\n",
    "                spg_policy, spg_value = policy[i], value[i]\n",
    "\n",
    "                valid_moves = self.game.get_valid_moves(node.state)\n",
    "                spg_policy *= valid_moves\n",
    "                # spg_policy /= np.sum(spg_policy)\n",
    "                spg_sum = np.sum(spg_policy)\n",
    "                if spg_sum == 0:\n",
    "                    spg_policy = valid_moves.astype(np.float32)\n",
    "                    spg_policy /=np.sum(valid_moves)\n",
    "                else:\n",
    "                    spg_policy /= sum_spg\n",
    "\n",
    "                node.expand(spg_policy)\n",
    "                node.backpropagate(spg_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaZeroParallel:\n",
    "    def __init__(self, model, optimizer, game, args):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.mcts = MCTSParallel(game, args, model)\n",
    "\n",
    "    def self_play(self):\n",
    "        return_memory = []\n",
    "        player = 1\n",
    "        self_play_games = [SPG(self.game) for _ in range(self.args['num_parallel_games'])]\n",
    "\n",
    "        while len(self_play_games) > 0:\n",
    "            states = np.stack([spg.state for spg in self_play_games])\n",
    "\n",
    "            neutral_states = self.game.change_perspective(states, player)\n",
    "            \n",
    "            self.mcts.search(neutral_states, self_play_games)\n",
    "\n",
    "            for i in range(len(self_play_games))[::-1]:\n",
    "                spg = self_play_games[i]\n",
    "                \n",
    "\n",
    "                # return visit_counts\n",
    "                action_probs = np.zeros(self.game.action_size)\n",
    "                for child in spg.root.children:\n",
    "                    action_probs[child.action_taken] = child.visit_count\n",
    "                action_probs /= np.sum(action_probs)\n",
    "\n",
    "                spg.memory.append((spg.root.state, action_probs, player))\n",
    "\n",
    "                temperature_action_probs = action_probs ** (1 / self.args['temperature'])\n",
    "                # temperature_action_probs /= np.sum(temperature_action_probs)\n",
    "                if np.sum(temperature_action_probs) == 0:\n",
    "                    temperature_action_probs = np.ones_like(temperature_action_probs) / len(temperature_action_probs)\n",
    "                else:\n",
    "                    temperature_action_probs /= np.sum(temperature_action_probs)\n",
    "\n",
    "\n",
    "                # changed to temp_probs, but video has action_probs\n",
    "                action = np.random.choice(self.game.action_size, p=temperature_action_probs)\n",
    "                \n",
    "                spg.state = self.game.get_next_state(spg.state, action, player)\n",
    "\n",
    "                value, is_terminal = self.game.get_value_and_terminated(spg.state, action)\n",
    "\n",
    "                if is_terminal:\n",
    "                    for hist_neutral_state, hist_action_probs, hist_player in spg.memory:\n",
    "                        hist_outcome = (value if hist_player == player else self.game.get_opponent_value(value))\n",
    "                        return_memory.append((\n",
    "                            self.game.get_encoded_state(hist_neutral_state),\n",
    "                            hist_action_probs,\n",
    "                            hist_outcome\n",
    "                        ))\n",
    "                    del self_play_games[i]\n",
    "            \n",
    "            player = self.game.get_opponent(player)\n",
    "            return return_memory\n",
    "\n",
    "\n",
    "    def train(self, memory):\n",
    "        random.shuffle(memory)\n",
    "        for batch_idx in range(0, len(memory), self.args['batch_size']):\n",
    "            sample = memory[batch_idx:min(len(memory) - 1, batch_idx + self.args['batch_size'])]\n",
    "            state, policy_targets, value_targets = zip(*sample)\n",
    "            state, policy_targets, value_targets = np.array(state), np.array(policy_targets), np.array(value_targets).reshape(-1, 1)\n",
    "            \n",
    "            state = torch.tensor(state, dtype=torch.float32, device=self.model.device)\n",
    "            policy_targets = torch.tensor(policy_targets, dtype=torch.float32, device=self.model.device)\n",
    "            value_targets = torch.tensor(value_targets, dtype=torch.float32, device=self.model.device)\n",
    "\n",
    "            out_policy, out_value = self.model(state)\n",
    "            \n",
    "            policy_loss = F.cross_entropy(out_policy, policy_targets)\n",
    "            value_loss = F.mse_loss(out_value, value_targets)\n",
    "            loss = policy_loss + value_loss\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "\n",
    "    def learn(self):\n",
    "        for i in range(self.args['num_iterations']):\n",
    "            memory = []\n",
    "            self.model.eval()\n",
    "            for self_play_iteration in tqdm(range(self.args['num_self_play_iterations'] // self.args['num_parallel_games'])):\n",
    "                memory += self.self_play()\n",
    "\n",
    "            self.model.train()\n",
    "            for eopch in tqdm(range(self.args['num_epochs'])):\n",
    "                self.train(memory)\n",
    "\n",
    "            torch.save(self.model.state_dict(), f\"weights/model_{i}_{self.game}.pt\")\n",
    "            torch.save(self.optimizer.state_dict(), f\"weights/optimizer_{i}_{self.game}.pt\")\n",
    "\n",
    "\n",
    "class SPG: # self play game\n",
    "    def __init__(self, game):\n",
    "        self.state = game.get_initial_state()\n",
    "        self.memory = []\n",
    "        self.root = None\n",
    "        self.node = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]/tmp/ipykernel_15243/3838911228.py:30: RuntimeWarning: invalid value encountered in divide\n",
      "  action_probs /= np.sum(action_probs)\n",
      "  0%|          | 0/5 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "probabilities contain NaN",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m      7\u001b[39m args = {\n\u001b[32m      8\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mC\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m2\u001b[39m,\n\u001b[32m      9\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mnum_searches\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m600\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     17\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mdirichlet_alpha\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m0.314159265358979\u001b[39m\n\u001b[32m     18\u001b[39m }\n\u001b[32m     20\u001b[39m alpha_zero = AlphaZeroParallel(model, optimizer, game, args)\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[43malpha_zero\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 90\u001b[39m, in \u001b[36mAlphaZeroParallel.learn\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     88\u001b[39m \u001b[38;5;28mself\u001b[39m.model.eval()\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m self_play_iteration \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.args[\u001b[33m'\u001b[39m\u001b[33mnum_self_play_iterations\u001b[39m\u001b[33m'\u001b[39m] // \u001b[38;5;28mself\u001b[39m.args[\u001b[33m'\u001b[39m\u001b[33mnum_parallel_games\u001b[39m\u001b[33m'\u001b[39m])):\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m     memory += \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself_play\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[38;5;28mself\u001b[39m.model.train()\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m eopch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.args[\u001b[33m'\u001b[39m\u001b[33mnum_epochs\u001b[39m\u001b[33m'\u001b[39m])):\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 43\u001b[39m, in \u001b[36mAlphaZeroParallel.self_play\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     39\u001b[39m     temperature_action_probs /= np.sum(temperature_action_probs)\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# changed to temp_probs, but video has action_probs\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m action = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrandom\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchoice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgame\u001b[49m\u001b[43m.\u001b[49m\u001b[43maction_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemperature_action_probs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m spg.state = \u001b[38;5;28mself\u001b[39m.game.get_next_state(spg.state, action, player)\n\u001b[32m     47\u001b[39m value, is_terminal = \u001b[38;5;28mself\u001b[39m.game.get_value_and_terminated(spg.state, action)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mnumpy/random/mtrand.pyx:990\u001b[39m, in \u001b[36mnumpy.random.mtrand.RandomState.choice\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m: probabilities contain NaN"
     ]
    }
   ],
   "source": [
    "game = ConnectFour()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = ResNet(game, 9, 128, device=device)\n",
    "optimizer = Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "\n",
    "args = {\n",
    "    'C': 2,\n",
    "    'num_searches': 600,\n",
    "    'num_iterations': 8,\n",
    "    'num_self_play_iterations': 500,\n",
    "    \"num_parallel_games\": 100,\n",
    "    'num_epochs': 5,\n",
    "    'batch_size': 128,\n",
    "    'temperature': 1.25,\n",
    "    'dirichlet_epsilon': 0.25,\n",
    "    'dirichlet_alpha': 0.314159265358979\n",
    "}\n",
    "\n",
    "alpha_zero = AlphaZeroParallel(model, optimizer, game, args)\n",
    "alpha_zero.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9856092929840088\n",
      "[[ 0.  0. -1.]\n",
      " [ 0. -1.  0.]\n",
      " [ 1.  0.  1.]]\n",
      "tensor([[[[0., 0., 1.],\n",
      "          [0., 1., 0.],\n",
      "          [0., 0., 0.]],\n",
      "\n",
      "         [[1., 1., 0.],\n",
      "          [1., 0., 1.],\n",
      "          [0., 1., 0.]],\n",
      "\n",
      "         [[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [1., 0., 1.]]]], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tictactoe = TicTacToe()\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "\n",
    "state = tictactoe.get_initial_state()\n",
    "state = tictactoe.get_next_state(state, 2, -1)\n",
    "state = tictactoe.get_next_state(state, 4, -1)\n",
    "state = tictactoe.get_next_state(state, 6, 1)\n",
    "state = tictactoe.get_next_state(state, 8, 1)\n",
    "\n",
    "\n",
    "encoded_state = tictactoe.get_encoded_state(state)\n",
    "tensor_state = torch.tensor(encoded_state, device=device).unsqueeze(0)\n",
    "model = ResNet(tictactoe, num_resBlocks=4, num_hidden=64, device=device)\n",
    "model.load_state_dict(torch.load(\"model_2.pt\", map_location=device))\n",
    "model.eval()\n",
    "\n",
    "policy, value = model(tensor_state)\n",
    "value = value.item()\n",
    "policy = torch.softmax(policy, dim=1).squeeze(0).detach().cpu().numpy()\n",
    "print(value)\n",
    "print(state)\n",
    "print(tensor_state)\n",
    "\n",
    "plt.bar(range(tictactoe.action_size), policy)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]]\n",
      "valid moves [0, 1, 2, 3, 4, 5, 6]\n",
      "[[0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0. -1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.]]\n",
      "valid moves [0, 1, 2, 3, 4, 5, 6]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0. -1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.]]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0. -1.  0.  0.  0.]\n",
      " [ 0. -1.  0.  1.  0.  0.  0.]]\n",
      "valid moves [0, 1, 2, 3, 4, 5, 6]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0. -1.  0.  0.  0.]\n",
      " [ 0. -1.  0.  1.  0.  0.  1.]]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0. -1.  0.  0. -1.]\n",
      " [ 0. -1.  0.  1.  0.  0.  1.]]\n",
      "valid moves [0, 1, 2, 3, 4, 5, 6]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0. -1.  0.  0. -1.]\n",
      " [ 1. -1.  0.  1.  0.  0.  1.]]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0. -1.  0.  0. -1.]\n",
      " [ 1. -1. -1.  1.  0.  0.  1.]]\n",
      "valid moves [0, 1, 2, 3, 4, 5, 6]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 1.  0.  0. -1.  0.  0. -1.]\n",
      " [ 1. -1. -1.  1.  0.  0.  1.]]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 1.  0.  0. -1.  0.  0. -1.]\n",
      " [ 1. -1. -1.  1.  0. -1.  1.]]\n",
      "valid moves [0, 1, 2, 3, 4, 5, 6]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  1.  0.  0.  0.]\n",
      " [ 1.  0.  0. -1.  0.  0. -1.]\n",
      " [ 1. -1. -1.  1.  0. -1.  1.]]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  1.  0.  0.  0.]\n",
      " [ 1.  0.  0. -1.  0.  0. -1.]\n",
      " [ 1. -1. -1.  1. -1. -1.  1.]]\n",
      "valid moves [0, 1, 2, 3, 4, 5, 6]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  1.  0.  0.  0.]\n",
      " [ 1.  0.  0. -1.  0.  0. -1.]\n",
      " [ 1. -1. -1.  1. -1. -1.  1.]]\n",
      "1 won\n"
     ]
    }
   ],
   "source": [
    "game = ConnectFour()\n",
    "player = 1\n",
    "state = game.get_initial_state()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "args = {\n",
    "    'C': 2,\n",
    "    'num_searches': 1000,\n",
    "    'dirichlet_epsilon': 0.25,\n",
    "    'dirichlet_alpha': 0.3\n",
    "}\n",
    "\n",
    "torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = ResNet(game, num_resBlocks=9, num_hidden=128, device=device)\n",
    "model.load_state_dict(torch.load(\"weights/model_0_ConnectFour.pt\", map_location=device))\n",
    "model.eval()\n",
    "\n",
    "mcts = MCTS(game, args, model)\n",
    "\n",
    "while True:\n",
    "    print(state)\n",
    "    if player == 1:\n",
    "\n",
    "        valid_moves = game.get_valid_moves(state)\n",
    "        print(\"valid moves\", [i for i in range(\n",
    "            game.action_size) if valid_moves[i] == 1])\n",
    "        action = int(input(f\"{player}: \"))\n",
    "\n",
    "        if valid_moves[action] == 0:\n",
    "            print(\"Action not valid\")\n",
    "            continue\n",
    "\n",
    "    else:\n",
    "        neutral_state = game.change_perspective(state, player)\n",
    "        mcts_probs = mcts.search(neutral_state)\n",
    "        action = np.argmax(mcts_probs)\n",
    "\n",
    "    state = game.get_next_state(state, action, player)\n",
    "\n",
    "    value, is_terminal = game.get_value_and_terminated(\n",
    "        state, \n",
    "        action)\n",
    "\n",
    "    if is_terminal:\n",
    "        print(state)\n",
    "        if value == 1:\n",
    "            print(f\"{player} won\")\n",
    "        else:\n",
    "            print(\"draw\")\n",
    "        break    \n",
    " \n",
    "    player = game.get_opponent(player)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
